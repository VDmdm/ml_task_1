{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавливаем зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ischedule\n",
      "  Downloading ischedule-1.2.7-py3-none-any.whl (5.3 kB)\n",
      "Requirement already satisfied: requests in /home/snelk/.local/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/snelk/.local/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/snelk/.local/lib/python3.10/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/snelk/.local/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/snelk/.local/lib/python3.10/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/snelk/.local/lib/python3.10/site-packages (from requests) (2023.11.17)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/snelk/.local/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Installing collected packages: ischedule\n",
      "Successfully installed ischedule-1.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip3 install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем базу и применяем миграции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly connect to db openfish_parsing_data.db\n",
      "Successfuly create tables if not exists in openfish_parsing_data.db\n"
     ]
    }
   ],
   "source": [
    "import sqlite3, os\n",
    "\n",
    "create_table_statement = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS parse_iteration_info(\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        started_at DATETIME DEFAULT CURRENT_TIMESTAMP,\n",
    "        ended_at DATETIME DEFAULT NULL,\n",
    "        successfully BOOL DEFAULT FALSE,\n",
    "        errors TEXT DEAFULT \"\"\n",
    "    );\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS phishing_urls (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "        url TEXT,\n",
    "        brand VARCHAR(256),\n",
    "        time DATETIME,\n",
    "        iterationId INTEGER NOT NULL,\n",
    "        FOREIGN KEY(iterationId) REFERENCES parse_iteration_info(id)\n",
    "    );\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "def create_table(conn):\n",
    "    cur = conn.cursor()\n",
    "    for st in create_table_statement:\n",
    "        res = cur.execute(st)\n",
    "    cur.close()\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def create_sqlite_database(filename):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(filename)\n",
    "    except sqlite3.Error as e:\n",
    "        print(e)\n",
    "    return conn\n",
    "\n",
    "db_name = \"openfish_parsing_data.db\"\n",
    "\n",
    "conn = create_sqlite_database(db_name)\n",
    "if conn == None:\n",
    "    print(f\"Error: Can't connect to db {db_name}\")\n",
    "    os.exit(1)\n",
    "\n",
    "print(f\"Successfuly connect to db {db_name}\")\n",
    "\n",
    "create_table(conn)\n",
    "print(f\"Successfuly create tables if not exists in {db_name}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем парсер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New parse iteration started at 2024-09-15 21:41:01.365476+00:00\n",
      "Last time of parse is 2024-09-15 21:16:07.607605+00:00\n",
      "Parse iteration ID is 242\n",
      "Successfully got 30 new urls from site\n",
      "Keep 10 after filter by time and unique\n",
      "Parse iteration finished successfully\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 217\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     start_parse_iteration()\n\u001b[0;32m--> 217\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sqlite3, time, traceback, pytz\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import requests as re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "db_name = \"openfish_parsing_data.db\"\n",
    "\n",
    "def create_parse_iteration_info(conn, start_time):\n",
    "    try:\n",
    "        query = \"INSERT INTO parse_iteration_info (started_at)  VALUES (?)\"\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query , (start_time,))\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"Failed when try to deal sql query: {query}\")\n",
    "        return e\n",
    "\n",
    "    return cur.lastrowid\n",
    "\n",
    "\n",
    "def update_parse_iteration_info(conn, id, ended_time = datetime.now(timezone.utc), succesfully = True, error_text = \"\"):\n",
    "    try:\n",
    "        query = \"UPDATE parse_iteration_info SET ended_at = ?, successfully = ?, errors = ? where id = ?\"\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query , (ended_time, succesfully, error_text, id))\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"Failed when try to deal sql query: {query}\")\n",
    "        return e\n",
    "\n",
    "    return cur.lastrowid\n",
    "    \n",
    "\n",
    "def get_parse_iteration_info_last_timestamp(conn):\n",
    "    try:\n",
    "        query = \"SELECT ended_at FROM parse_iteration_info WHERE successfully ORDER BY id DESC LIMIT 1\"\n",
    "        cur = conn.cursor()\n",
    "        res = cur.execute(query)\n",
    "        ts = res.fetchall()\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"Failed when try to deal sql query: {query}\")\n",
    "        return e\n",
    "\n",
    "    if len(ts) == 0:\n",
    "        return None\n",
    "    \n",
    "    ts = datetime.strptime(ts[0][0], \"%Y-%m-%d %H:%M:%S.%f%z\").replace(tzinfo=pytz.utc)\n",
    "    return ts\n",
    "\n",
    "def create_phishing_urls(conn, data):\n",
    "    try:\n",
    "        query = \"INSERT INTO phishing_urls (url, brand, time, iterationId)  VALUES (?, ?, ?, ?)\"\n",
    "        cur = conn.cursor()\n",
    "        cur.executemany(query , data)\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"Failed when try to deal sql query: {query}\")\n",
    "        return e\n",
    "    \n",
    "    return cur.lastrowid\n",
    "\n",
    "def select_phising_urls(conn, url, brand, time=None):\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "        if time == None:\n",
    "            query = \"SELECT * FROM phishing_urls WHERE url=? AND brand=?\"\n",
    "            res = cur.execute(query, (url, brand,)).fetchall()\n",
    "        else:\n",
    "            query = \"SELECT * FROM phishing_urls WHERE url=? AND brand=? AND time=?\"\n",
    "            res = cur.execute(query, (url, brand, time)).fetchall()\n",
    "        cur.close()\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"Failed when try to deal sql query: {query}\")\n",
    "        return e\n",
    "    \n",
    "    return res\n",
    "    \n",
    "def get_html(url):\n",
    "    headers = {\n",
    "        \"Accept\": \"text/html\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_3_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15\"\n",
    "    }\n",
    "    req = re.get(url, headers)\n",
    "    if req.status_code != 200:\n",
    "        print(f\"Status code != 200. Url: {url}, status_code: {req.status_code}\")\n",
    "        return \"\"\n",
    "    return req.text\n",
    "\n",
    "def parse_data(html):\n",
    "    url_data = []\n",
    "\n",
    "    try:\n",
    "        soup = bs(html)\n",
    "        table_rows = soup.find(\"table\", class_=\"pure-table pure-table-striped\").find(\"tbody\").find_all(\"tr\")\n",
    "        # tbody = table.find(\"tbody\")\n",
    "        for table_row in table_rows:\n",
    "            tds = table_row.find_all(\"td\")\n",
    "            url, brand, time = tds[0].string, tds[1].string, tds[2].string\n",
    "            time_formated = datetime.strptime(time, \"%H:%M:%S\")\n",
    "            time_formated = datetime.now(timezone.utc).replace(hour=time_formated.hour, minute=time_formated.minute, second=time_formated.second, microsecond=0)\n",
    "            \n",
    "            # на случай если запуск придется на переходный период между сутками\n",
    "            if time_formated > datetime.now(timezone.utc):\n",
    "                time_formated = time_formated - timedelta(days=1)\n",
    "\n",
    "            url_data.append({\"url\": url, \"brand\": brand, \"time_formated\": time_formated})\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "        return e\n",
    "\n",
    "    return url_data\n",
    "\n",
    "def start_parse_iteration():\n",
    "    conn = sqlite3.connect(db_name)\n",
    "\n",
    "    start_datetime = datetime.now(timezone.utc)\n",
    "    print(f\"New parse iteration started at {start_datetime}\")\n",
    "\n",
    "    last_time = get_parse_iteration_info_last_timestamp(conn)\n",
    "\n",
    "    if last_time == sqlite3.Error:\n",
    "        print(f\"Can't start parsing due error: {last_time}\")\n",
    "        conn.close()\n",
    "        return\n",
    "    \n",
    "    print(f\"Last time of parse is {last_time}\")\n",
    "    \n",
    "    id = create_parse_iteration_info(conn, start_datetime)\n",
    "    if id == sqlite3.Error:\n",
    "        last_id = update_parse_iteration_info(id, succesfully=False, error_text=id)\n",
    "        if last_id == sqlite3.Error:\n",
    "            print(\"Can't write failed status to db.\")\n",
    "        conn.close()\n",
    "        return\n",
    "    \n",
    "    print(f\"Parse iteration ID is {id}\")\n",
    "    \n",
    "    html = get_html(\"https://openphish.com/\")\n",
    "    if html == \"\":\n",
    "        print(f\"Can't parse data from site. Error: Empty html\")\n",
    "        last_id = update_parse_iteration_info(id, succesfully=False, error_text=\"Empty html\")\n",
    "        if last_id == sqlite3.Error:\n",
    "            print(\"Can't write failed status to db.\")\n",
    "        conn.close()\n",
    "        return\n",
    "    \n",
    "    url_data = parse_data(html)\n",
    "    if not isinstance(url_data, list):\n",
    "        print(f\"Can't parse data from site. Error: {url_data}\")\n",
    "        last_id = update_parse_iteration_info(id, succesfully=False, error_text=url_data)\n",
    "        if last_id == sqlite3.Error:\n",
    "            print(\"Can't write failed status to db.\")\n",
    "        conn.close()\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully got {len(url_data)} new urls from site\")\n",
    "    \n",
    "    query_data = []\n",
    "\n",
    "    for data in url_data:\n",
    "        if last_time is not None and data['time_formated'] < last_time:\n",
    "            continue\n",
    "\n",
    "        existing_url = select_phising_urls(conn, data['url'], data['brand'])\n",
    "        if existing_url == sqlite3.Error:\n",
    "            print(\"Can't check unique.\")\n",
    "            last_id = update_parse_iteration_info(id, succesfully=False, error_text=existing_url)\n",
    "            if last_id == sqlite3.Error:\n",
    "                print(\"Can't write failed status to db.\")\n",
    "            conn.close()\n",
    "            return\n",
    "         \n",
    "        if len(existing_url) != 0:\n",
    "            continue\n",
    "\n",
    "        query_data.append((data['url'], data['brand'], data['time_formated'], id))\n",
    "    \n",
    "    print(f\"Keep {len(query_data)} after filter by time and unique\")\n",
    "    \n",
    "    if len(query_data) > 0:\n",
    "        last_id = create_phishing_urls(conn, query_data)\n",
    "        if last_id == sqlite3.Error:\n",
    "            print(\"Can't write urls to db.\")\n",
    "            last_id = update_parse_iteration_info(id, succesfully=False, error_text=last_id)\n",
    "            if last_id == sqlite3.Error:\n",
    "                print(\"Can't write failed status to db.\")\n",
    "            conn.close()\n",
    "            return\n",
    "\n",
    "\n",
    "    \n",
    "    last_id = update_parse_iteration_info(conn, id, ended_time=datetime.now(timezone.utc))\n",
    "    if last_id == sqlite3.Error:\n",
    "        print(\"Can't write success status to db.\")\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Parse iteration finished successfully\")\n",
    "\n",
    "while True:\n",
    "    start_parse_iteration()\n",
    "    time.sleep(300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем необходимую информацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты работы:\n",
      "Время начала парсинга: 2024-09-14 22:18:48\n",
      "Время окончания парсинга: 2024-09-15 21:41:01\n",
      "Количество уникальных URL сайтов за данный период: 441\n",
      "Топ 3 Наиболее часто атакуемых брендов:\n",
      "1: Generic/Spear Phishing - 76\n",
      "2: Crypto/Wallet - 48\n",
      "3: Facebook, Inc. - 32\n"
     ]
    }
   ],
   "source": [
    "import sqlite3, time, traceback, pytz\n",
    "\n",
    "conn = sqlite3.connect(db_name)\n",
    "\n",
    "query_get_parsing_time = \"\"\"\n",
    "SELECT MIN(started_at) AS f,\n",
    "       MAX(ended_at) AS l\n",
    "FROM parse_iteration_info;\n",
    "\"\"\"\n",
    "\n",
    "unique_urls_query = \"\"\"\n",
    "SELECT COUNT(*) FROM phishing_urls;\n",
    "\"\"\"\n",
    "\n",
    "top_attacking_brand_query = \"\"\"\n",
    "SELECT brand, COUNT(brand) FROM phishing_urls\n",
    "GROUP BY brand \n",
    "ORDER BY COUNT(brand) DESC\n",
    "LIMIT 3\n",
    "\"\"\"\n",
    "\n",
    "cur = conn.cursor()\n",
    "parsing_time_query_result = cur.execute(query_get_parsing_time).fetchall()\n",
    "cur.close()\n",
    "\n",
    "cur = conn.cursor()\n",
    "unique_urls_query_result = cur.execute(unique_urls_query).fetchall()\n",
    "cur.close()\n",
    "\n",
    "cur = conn.cursor()\n",
    "top_attacking_brand_query_result = cur.execute(top_attacking_brand_query).fetchall()\n",
    "cur.close()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "parsing_start_time = datetime.strptime(parsing_time_query_result[0][0], \"%Y-%m-%d %H:%M:%S.%f%z\").replace(tzinfo=pytz.utc)\n",
    "parsing_end_time = datetime.strptime(parsing_time_query_result[0][1], \"%Y-%m-%d %H:%M:%S.%f%z\").replace(tzinfo=pytz.utc)\n",
    "print(\"Результаты работы:\")\n",
    "print(f\"Время начала парсинга: {parsing_start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Время окончания парсинга: {parsing_end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Количество уникальных URL сайтов за данный период: {unique_urls_query_result[0][0]}\")\n",
    "print(\"Топ 3 Наиболее часто атакуемых брендов:\")\n",
    "i = 1\n",
    "for d in top_attacking_brand_query_result:\n",
    "    print(f'{i}: {d[0]} - {d[1]}')\n",
    "    i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
